{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 479.6 MB 29 kB/s s eta 0:00:01     |██████████████████████████████▌ | 457.2 MB 12.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 14 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 47 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 874 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 883 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from tensorflow) (44.0.0)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[K     |████████████████████████████████| 440 kB 53.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.62.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 29.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 60.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<=1.24.3,>=1.22\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 9.6 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 67.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 46.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from tensorflow) (23.2)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 79.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 302 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[K     |████████████████████████████████| 189 kB 80.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 33.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 31.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (7.1.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 302 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/leonardo.miyagi/Documents/pos ia/.venv/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.18.1)\n",
      "\u001b[31mERROR: torch 2.2.2 has requirement typing-extensions>=4.8.0, but you'll have typing-extensions 4.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sqlalchemy 2.0.29 has requirement typing-extensions>=4.6.0, but you'll have typing-extensions 4.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pydantic-core 2.16.3 has requirement typing-extensions!=4.7.0,>=4.6.0, but you'll have typing-extensions 4.5.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wrapt, numpy, h5py, opt-einsum, typing-extensions, google-pasta, flatbuffers, tensorflow-estimator, wheel, astunparse, gast, grpcio, protobuf, absl-py, libclang, tensorflow-io-gcs-filesystem, tensorboard-data-server, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, markdown, tensorboard, termcolor, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.10.0\n",
      "    Uninstalling typing-extensions-4.10.0:\n",
      "      Successfully uninstalled typing-extensions-4.10.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.29.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-2.13.1 libclang-18.1.1 markdown-3.6 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 typing-extensions-4.5.0 werkzeug-3.0.2 wheel-0.43.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['IA supera humanos em compreensão de texto pela primeira vez.', 'Inaugurado o maior parque solar da América Latina.', 'Vacina contra a dengue mostra eficácia de em testes.', 'Desmatamento na Amazônia cai em um ano, diz governo.', 'Robôs agora capazes de realizar cirurgias delicadas do coração.', 'Cientistas criam plástico que se decompõe à luz do sol.', 'Jovem de 17,anos vence campeonato mundial de xadrez.', 'Lei de igualdade salarial entra em vigor, garantindo equidade.', 'Primeiro trem movido a hidrogênio faz viagem inaugural.', 'Líderes globais se reúnem para discutir a crise climática.', 'Tecnologia blockchain promete maior transparência eleitoral.', 'Recifes de coral são restaurados com técnica inovadora.', 'Cidade proíbe carros no centro para combater poluição.', 'Aumento significativo no financiamento para pesquisas de câncer.', 'Mudanças na política de imigração prometem acelerar processos.', 'Atleta brasileiro quebra recorde mundial nos rasos.', 'Nova espécie de tubarão descoberta em águas profundas.', 'Eleições presidenciais serão totalmente digitais pela primeira vez.', 'Avanço em IA pode prever terremotos com precisão inédita.', 'Festival de música clássica online alcança audiência recorde.', 'Museu do Louvre inaugura nova ala dedicada à arte moderna.', 'Protestos exigem ações concretas contra mudanças climáticas.', 'Dieta mediterrânea reduz risco de doenças cardíacas, mostra estudo.', 'Jogo de realidade virtual ajuda na reabilitação de pacientes AVC.', 'Nova lei obriga empresas a reciclar de seus resíduos.', 'Série documental revela a vida secreta dos animais noturnos.', 'Empresa lança carro elétrico com autonomia de 1000,km.', 'Novo tratamento para Alzheimer restaura memória em testes.', 'Astrônomos descobrem planeta semelhante à Terra em zona habitável.', 'Escola adota realidade aumentada para ensinar história.', 'Corrida espacial: empresa anuncia viagem comercial à Lua.', 'Parque eólico no mar bate recorde de produção de energia.', 'Governo aumenta investimento em energia limpa em 50,.', 'Estudo revela aumento da biodiversidade em áreas protegidas.', 'Cientistas desenvolvem vacina universal contra a gripe.', 'Festival de cinema independente foca em diretores estreantes.', 'História de superação de atleta paralímpico vira filme.', 'Reforma educacional promove aprendizado baseado em projetos.', 'Lançado o primeiro smartphone com tela dobrável.', 'Reserva natural é ampliada para proteger espécies ameaçadas.', 'Nova terapia genética oferece esperança contra a cegueira.', 'Revelados novos manuscritos do Mar Morto.', 'Painéis solares transparentes revolucionam arquitetura urbana.', 'Livro de poesias sobre mudanças climáticas ganha prêmio internacional.', 'Recordes de temperatura evidenciam urgência climática.', 'Desenvolvido arroz geneticamente modificado que resiste a inundações.', 'Atleta conquista ouro em natação com técnica revolucionária.', 'Manifestações pacíficas pedem reforma na lei de direitos autorais.', 'Artista usa IA para criar obras de arte inovadoras.', 'Congresso de tecnologia apresenta avanços na robótica.']\n",
      "Labels: ['tecnologia', 'meio ambiente', 'saúde', 'meio ambiente', 'tecnologia', 'ciência', 'esportes', 'política', 'tecnologia', 'política', 'tecnologia', 'meio ambiente', 'meio ambiente', 'saúde', 'política', 'esportes', 'ciência', 'política', 'tecnologia', 'cultura', 'cultura', 'política', 'saúde', 'tecnologia', 'meio ambiente', 'cultura', 'tecnologia', 'saúde', 'ciência', 'tecnologia', 'tecnologia', 'meio ambiente', 'política', 'meio ambiente', 'saúde', 'cultura', 'cultura', 'política', 'tecnologia', 'meio ambiente', 'saúde', 'cultura', 'tecnologia', 'cultura', 'meio ambiente', 'ciência', 'esportes', 'política', 'cultura', 'tecnologia']\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 361ms/step - loss: 1.9468 - accuracy: 0.1000 - val_loss: 1.9446 - val_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.9420 - accuracy: 0.3250 - val_loss: 1.9428 - val_accuracy: 0.2000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.9421 - accuracy: 0.1500 - val_loss: 1.9419 - val_accuracy: 0.2000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.9380 - accuracy: 0.3000 - val_loss: 1.9403 - val_accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.9377 - accuracy: 0.2500 - val_loss: 1.9387 - val_accuracy: 0.2000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.9313 - accuracy: 0.2250 - val_loss: 1.9376 - val_accuracy: 0.2000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.9296 - accuracy: 0.2500 - val_loss: 1.9367 - val_accuracy: 0.2000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.9288 - accuracy: 0.2250 - val_loss: 1.9357 - val_accuracy: 0.2000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.9283 - accuracy: 0.2750 - val_loss: 1.9341 - val_accuracy: 0.2000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.9237 - accuracy: 0.2000 - val_loss: 1.9318 - val_accuracy: 0.2000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.9152 - accuracy: 0.2750 - val_loss: 1.9291 - val_accuracy: 0.2000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.9180 - accuracy: 0.1000 - val_loss: 1.9259 - val_accuracy: 0.2000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.9124 - accuracy: 0.2500 - val_loss: 1.9229 - val_accuracy: 0.2000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.9107 - accuracy: 0.2000 - val_loss: 1.9212 - val_accuracy: 0.2000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.9142 - accuracy: 0.2000 - val_loss: 1.9203 - val_accuracy: 0.2000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.8864 - accuracy: 0.3000 - val_loss: 1.9181 - val_accuracy: 0.2000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.8895 - accuracy: 0.2000 - val_loss: 1.9191 - val_accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.8650 - accuracy: 0.2500 - val_loss: 1.9230 - val_accuracy: 0.2000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.8819 - accuracy: 0.2250 - val_loss: 1.9273 - val_accuracy: 0.2000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.8742 - accuracy: 0.2750 - val_loss: 1.9324 - val_accuracy: 0.2000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.8517 - accuracy: 0.1750 - val_loss: 1.9410 - val_accuracy: 0.2000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.8319 - accuracy: 0.2250 - val_loss: 1.9519 - val_accuracy: 0.2000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.8631 - accuracy: 0.1750 - val_loss: 1.9681 - val_accuracy: 0.2000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.7756 - accuracy: 0.2750 - val_loss: 2.0026 - val_accuracy: 0.2000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.7502 - accuracy: 0.2500 - val_loss: 2.0314 - val_accuracy: 0.2000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.8207 - accuracy: 0.2250 - val_loss: 2.0358 - val_accuracy: 0.2000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.7390 - accuracy: 0.1500 - val_loss: 2.0337 - val_accuracy: 0.2000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.5696 - accuracy: 0.2750 - val_loss: 2.0657 - val_accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.6354 - accuracy: 0.3000 - val_loss: 2.0777 - val_accuracy: 0.2000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.5347 - accuracy: 0.3750 - val_loss: 2.0797 - val_accuracy: 0.2000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.4979 - accuracy: 0.3750 - val_loss: 2.0734 - val_accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4106 - accuracy: 0.4250 - val_loss: 2.1773 - val_accuracy: 0.2000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4491 - accuracy: 0.3500 - val_loss: 2.1952 - val_accuracy: 0.3000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.3994 - accuracy: 0.3750 - val_loss: 2.2368 - val_accuracy: 0.3000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.4186 - accuracy: 0.3500 - val_loss: 2.1349 - val_accuracy: 0.3000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.3759 - accuracy: 0.3750 - val_loss: 2.2600 - val_accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.2199 - accuracy: 0.4750 - val_loss: 2.6276 - val_accuracy: 0.4000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.2436 - accuracy: 0.4250 - val_loss: 2.8247 - val_accuracy: 0.4000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.3108 - accuracy: 0.3500 - val_loss: 2.5694 - val_accuracy: 0.3000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2934 - accuracy: 0.4000 - val_loss: 2.2852 - val_accuracy: 0.1000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.1606 - accuracy: 0.5750 - val_loss: 2.3186 - val_accuracy: 0.2000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.2407 - accuracy: 0.5000 - val_loss: 2.6737 - val_accuracy: 0.3000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.3311 - accuracy: 0.3750 - val_loss: 2.9531 - val_accuracy: 0.3000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.1905 - accuracy: 0.4000 - val_loss: 2.6825 - val_accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1177 - accuracy: 0.5000 - val_loss: 2.4114 - val_accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.1154 - accuracy: 0.5000 - val_loss: 2.5232 - val_accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1153 - accuracy: 0.5000 - val_loss: 2.9132 - val_accuracy: 0.1000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0844 - accuracy: 0.5750 - val_loss: 2.9172 - val_accuracy: 0.1000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.0260 - accuracy: 0.5750 - val_loss: 2.5169 - val_accuracy: 0.2000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.0728 - accuracy: 0.5250 - val_loss: 2.4402 - val_accuracy: 0.2000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0879 - accuracy: 0.5000 - val_loss: 2.5717 - val_accuracy: 0.2000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.0645 - accuracy: 0.6250 - val_loss: 2.8264 - val_accuracy: 0.2000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0535 - accuracy: 0.4750 - val_loss: 3.0864 - val_accuracy: 0.1000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0351 - accuracy: 0.6000 - val_loss: 3.0407 - val_accuracy: 0.1000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9407 - accuracy: 0.5500 - val_loss: 2.7946 - val_accuracy: 0.2000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9993 - accuracy: 0.6500 - val_loss: 2.6919 - val_accuracy: 0.2000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9455 - accuracy: 0.5750 - val_loss: 2.7952 - val_accuracy: 0.2000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0395 - accuracy: 0.5250 - val_loss: 3.1213 - val_accuracy: 0.1000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8728 - accuracy: 0.5500 - val_loss: 3.3303 - val_accuracy: 0.1000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9619 - accuracy: 0.6000 - val_loss: 3.3061 - val_accuracy: 0.1000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9236 - accuracy: 0.5500 - val_loss: 3.2154 - val_accuracy: 0.1000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7666 - accuracy: 0.7000 - val_loss: 3.1680 - val_accuracy: 0.1000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8992 - accuracy: 0.5750 - val_loss: 3.1901 - val_accuracy: 0.1000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9239 - accuracy: 0.6000 - val_loss: 3.2410 - val_accuracy: 0.1000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8847 - accuracy: 0.5500 - val_loss: 3.2604 - val_accuracy: 0.1000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9105 - accuracy: 0.6000 - val_loss: 3.3100 - val_accuracy: 0.1000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8818 - accuracy: 0.6250 - val_loss: 3.5158 - val_accuracy: 0.1000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9259 - accuracy: 0.6250 - val_loss: 3.7066 - val_accuracy: 0.1000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8894 - accuracy: 0.6250 - val_loss: 3.6844 - val_accuracy: 0.1000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8396 - accuracy: 0.6000 - val_loss: 3.7425 - val_accuracy: 0.1000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7375 - accuracy: 0.6500 - val_loss: 4.0309 - val_accuracy: 0.1000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7739 - accuracy: 0.6750 - val_loss: 4.3708 - val_accuracy: 0.1000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7859 - accuracy: 0.6500 - val_loss: 4.4669 - val_accuracy: 0.1000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7396 - accuracy: 0.6250 - val_loss: 4.5662 - val_accuracy: 0.1000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8191 - accuracy: 0.6750 - val_loss: 4.3454 - val_accuracy: 0.1000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7240 - accuracy: 0.7250 - val_loss: 4.1330 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8007 - accuracy: 0.7000 - val_loss: 4.4008 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7338 - accuracy: 0.7250 - val_loss: 4.9242 - val_accuracy: 0.1000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6825 - accuracy: 0.7500 - val_loss: 5.4442 - val_accuracy: 0.1000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6659 - accuracy: 0.7500 - val_loss: 5.5181 - val_accuracy: 0.1000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7277 - accuracy: 0.6500 - val_loss: 5.0564 - val_accuracy: 0.1000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6263 - accuracy: 0.8250 - val_loss: 4.8264 - val_accuracy: 0.1000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6641 - accuracy: 0.7500 - val_loss: 4.8415 - val_accuracy: 0.1000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7121 - accuracy: 0.7750 - val_loss: 5.4237 - val_accuracy: 0.1000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6370 - accuracy: 0.8000 - val_loss: 6.0954 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6685 - accuracy: 0.6750 - val_loss: 6.4542 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6389 - accuracy: 0.7500 - val_loss: 6.4941 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5740 - accuracy: 0.7500 - val_loss: 6.4481 - val_accuracy: 0.1000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5768 - accuracy: 0.8000 - val_loss: 6.3180 - val_accuracy: 0.1000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5947 - accuracy: 0.7750 - val_loss: 6.3790 - val_accuracy: 0.1000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6110 - accuracy: 0.7250 - val_loss: 6.5789 - val_accuracy: 0.1000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5583 - accuracy: 0.7250 - val_loss: 6.7968 - val_accuracy: 0.1000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5394 - accuracy: 0.7500 - val_loss: 6.9925 - val_accuracy: 0.1000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5947 - accuracy: 0.8000 - val_loss: 7.1275 - val_accuracy: 0.1000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6618 - accuracy: 0.7000 - val_loss: 7.1484 - val_accuracy: 0.1000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6101 - accuracy: 0.8000 - val_loss: 7.1668 - val_accuracy: 0.1000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5319 - accuracy: 0.7750 - val_loss: 7.2918 - val_accuracy: 0.1000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4918 - accuracy: 0.8250 - val_loss: 7.5109 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5341 - accuracy: 0.7750 - val_loss: 7.7453 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5147 - accuracy: 0.8500 - val_loss: 7.8619 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.8619 - accuracy: 0.0000e+00\n",
      "Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your actual CSV file\n",
    "file_path = 'sentencas.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame with \";\" delimiter\n",
    "df = pd.read_csv(file_path, header=None, usecols=[0, 1], sep=';')\n",
    "\n",
    "# Extract columns into lists\n",
    "sentences = df[0].tolist()\n",
    "labels = df[1].tolist()\n",
    "\n",
    "# Print the lists to verify\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load data\n",
    "#df = pd.read_csv('your_data.csv')  # Replace 'your_data.csv' with your file path\n",
    "#sentences = df['text'].values\n",
    "#labels = df['label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 20  # You can adjust this based on your maximum sequence length\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels))\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(test_padded, test_labels)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# You can also make predictions using the model\n",
    "# predictions = model.predict(test_padded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 305ms/step\n",
      "Sentence: O time venceu o campeonato após uma partida emocionante. - Predicted Label: tecnologia\n",
      "Sentence: A final da Copa do Mundo será realizada no Qatar. - Predicted Label: tecnologia\n",
      "Sentence: O jogador assinou um novo contrato com o clube europeu. - Predicted Label: tecnologia\n",
      "Sentence: A corrida de Fórmula 1 foi adiada devido à chuva forte. - Predicted Label: tecnologia\n",
      "Sentence: Essa foi a primeira medalha de ouro do país nas Olimpíadas. - Predicted Label: tecnologia\n",
      "Sentence: O atleta bateu o recorde mundial na competição de ontem. - Predicted Label: tecnologia\n",
      "Sentence: O governo anunciou novas medidas econômicas para combater a inflação. - Predicted Label: tecnologia\n",
      "Sentence: As eleições presidenciais serão realizadas em outubro. - Predicted Label: tecnologia\n",
      "Sentence: O senado aprovou a nova lei de segurança pública. - Predicted Label: tecnologia\n",
      "Sentence: O uso de drones para entrega de produtos está se tornando comum. - Predicted Label: tecnologia\n",
      "Sentence: A atualização do sistema operacional incluirá novos recursos de segurança. - Predicted Label: tecnologia\n",
      "Sentence: A feira de eletrônicos demonstrou um novo modelo de assistente doméstico inteligente. - Predicted Label: tecnologia\n",
      "Sentence: A rede social anunciou novas medidas para proteger a privacidade dos usuários. - Predicted Label: tecnologia\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already trained your model and loaded it into the variable 'model'\n",
    "\n",
    "# Load the new data\n",
    "new_df = pd.read_csv('novas.csv', header=None, usecols=[0])  # Replace 'new_data.csv' with your new dataset file path\n",
    "new_sentences = new_df[0].values\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_padded)\n",
    "\n",
    "# Decode predictions\n",
    "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Print predictions\n",
    "for sentence, label in zip(new_sentences, predicted_labels):\n",
    "    print(f'Sentence: {sentence} - Predicted Label: {label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
